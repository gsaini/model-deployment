{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "TiYIU8lPFGwt",
        "daBFxPKyAhrl",
        "BS5B2nU0GhzL",
        "T3XlDPUtJnDo",
        "STDSb04iT-rL",
        "JWD7rPCRUEtD",
        "UsCYxkq_UL3Q",
        "beq1RbMhUQmi",
        "h_SIZoHHMtqS",
        "vRqCTdwrRP6S",
        "uL3mB7rziwnz",
        "dPUTD0Snew7n",
        "3mU64zPDeyKR"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Problem Statement"
      ],
      "metadata": {
        "id": "TiYIU8lPFGwt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Business Context"
      ],
      "metadata": {
        "id": "daBFxPKyAhrl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the highly competitive telecommunications industry, customer retention is critical to sustaining growth and profitability. Customer churn remains a persistent challenge, making it essential to understand customer behavior and the underlying factors that drive their decision to leave. Gaining these insights is key to maintaining loyalty and delivering superior service.\n",
        "\n",
        "To tackle this, the Customer Analytics & Retention Department has been analyzing historical customer data, segmented into two main groups—customers who have churned and those who have been retained. By leveraging advanced machine learning techniques, the team has developed a predictive model to identify patterns associated with churn risk. The goal is to provide actionable insights that enable proactive, targeted retention strategies.\n",
        "\n",
        "However, several challenges hinder the effectiveness of the current process:\n",
        "\n",
        "1. **Data Overload**: The vast volume and complexity of customer data make it difficult to extract timely, meaningful insights, slowing down decision-making and intervention.\n",
        "2. **Delayed Responses**: The existing churn analysis approach lacks the speed needed to respond to emerging trends, often resulting in missed retention opportunities.\n",
        "3. **Limited Accessibility**: For the model to be impactful, it must be easily accessible to all customer-facing teams. A user-friendly web application is essential to ensure its widespread adoption and practical use.\n"
      ],
      "metadata": {
        "id": "lrxH46GIDF5o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objective"
      ],
      "metadata": {
        "id": "BS5B2nU0GhzL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To ensure seamless and scalable access to the churn prediction system, the Customer Analytics & Retention Department seeks to improve how the model and its environment are deployed across teams. The current centralized web-based deployment has caused performance issues and high latency as access expanded to distributed locations. Additionally, sharing the model has been problematic due to environment and system incompatibilities.\n",
        "\n",
        "The department's objective is to develop a standardized, portable solution that packages the model, dependencies, configurations, and runtime environment into a self-contained unit. This approach aims to eliminate compatibility issues, reduce errors during deployment, simplify setup, and enable reliable use of the application across different systems. The goal is to empower all teams with consistent, low-latency access to the churn prediction model, supporting timely and proactive customer retention efforts."
      ],
      "metadata": {
        "id": "Y3xRydv65ElY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# App Backend"
      ],
      "metadata": {
        "id": "QqdVZ2fkNylb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Points to note before executing the below cells\n",
        "- Visit [this](https://huggingface.co/new-space) link to create a new space\n",
        "  - Under the space creation, enter the below details\n",
        "    - Space name: **Backend**\n",
        "(If you were trying with different names, be cautious when using a underscore `_` in space names, such as `backend_space`, as it can cause exceptions when accessing the API URL. Always use hyphen `-` instead, like `backend-space`.)\n",
        "    - Select the space SDK: **Docker**\n",
        "    - Choose a Docker tempplate: **Blank**\n",
        "    - Click on **Create Space**"
      ],
      "metadata": {
        "id": "h7pUAZIhOPnD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Flask Web framework"
      ],
      "metadata": {
        "id": "T3XlDPUtJnDo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Flask is a lightweight, flexible Python web framework used to build web applications and APIs quickly and easily.\n",
        "\n",
        "Flask allows you to:\n",
        "- Create web routes (URLs that users can access)\n",
        "- Handle HTTP requests and responses\n",
        "- Build REST APIs to expose machine learning models or business logic\n",
        "- Serve HTML templates, form inputs, and JSON data\n",
        "\n",
        "This API allows any application (like a dashboard, CRM, or mobile app) to send customer data and get back a prediction on whether the customer is likely to churn."
      ],
      "metadata": {
        "id": "GHfzkhy3K9u5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a folder for storing the files needed for backend server deployment\n",
        "import os\n",
        "os.makedirs(\"backend_files\", exist_ok=True)"
      ],
      "metadata": {
        "id": "D4luHdaALN3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile backend_files/app.py\n",
        "import joblib\n",
        "import pandas as pd\n",
        "from flask import Flask, request, jsonify\n",
        "\n",
        "# Initialize Flask app with a name\n",
        "app = Flask(\"Telecom Customer Churn Predictor\")\n",
        "\n",
        "# Load the trained churn prediction model\n",
        "model = joblib.load(\"churn_prediction_model_v1_0.joblib\")\n",
        "\n",
        "# Define a route for the home page\n",
        "@app.get('/')\n",
        "def home():\n",
        "    return \"Welcome to the Telecom Customer Churn Prediction API\"\n",
        "\n",
        "# Define an endpoint to predict churn for a single customer\n",
        "@app.post('/v1/customer')\n",
        "def predict_churn():\n",
        "    # Get JSON data from the request\n",
        "    customer_data = request.get_json()\n",
        "\n",
        "    # Extract relevant customer features from the input data\n",
        "    sample = {\n",
        "        'SeniorCitizen': customer_data['SeniorCitizen'],\n",
        "        'Partner': customer_data['Partner'],\n",
        "        'Dependents': customer_data['Dependents'],\n",
        "        'tenure': customer_data['tenure'],\n",
        "        'PhoneService': customer_data['PhoneService'],\n",
        "        'InternetService': customer_data['InternetService'],\n",
        "        'Contract': customer_data['Contract'],\n",
        "        'PaymentMethod': customer_data['PaymentMethod'],\n",
        "        'MonthlyCharges': customer_data['MonthlyCharges'],\n",
        "        'TotalCharges': customer_data['TotalCharges']\n",
        "    }\n",
        "\n",
        "    # Convert the extracted data into a DataFrame\n",
        "    input_data = pd.DataFrame([sample])\n",
        "\n",
        "    # Make a churn prediction using the trained model\n",
        "    prediction = model.predict(input_data).tolist()[0]\n",
        "\n",
        "    # Map prediction result to a human-readable label\n",
        "    prediction_label = \"churn\" if prediction == 1 else \"not churn\"\n",
        "\n",
        "    # Return the prediction as a JSON response\n",
        "    return jsonify({'Prediction': prediction_label})\n",
        "\n",
        "# Define an endpoint to predict churn for a batch of customers\n",
        "@app.post('/v1/customerbatch')\n",
        "def predict_churn_batch():\n",
        "    # Get the uploaded CSV file from the request\n",
        "    file = request.files['file']\n",
        "\n",
        "    # Read the file into a DataFrame\n",
        "    input_data = pd.read_csv(file)\n",
        "\n",
        "    # Make predictions for the batch data and convert raw predictions into a readable format\n",
        "    predictions = [\n",
        "        'Churn' if x == 1\n",
        "        else \"Not Churn\"\n",
        "        for x in model.predict(input_data.drop(\"customerID\",axis=1)).tolist()\n",
        "    ]\n",
        "\n",
        "    cust_id_list = input_data.customerID.values.tolist()\n",
        "    output_dict = dict(zip(cust_id_list, predictions))\n",
        "\n",
        "    return output_dict\n",
        "\n",
        "# Run the Flask app in debug mode\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwUlhcXQFJs4",
        "outputId": "f9006cf3-7d4b-44c6-9074-dd99cf2e7a9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing backend_files/app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the given code snippet, the decorators `@app.get('/')`, `@app.post('/v1/customer')`, and `@app.post('/v1/customerbatch')` are used in **FlaskAPI**, a modern web framework for building APIs in Python.\n",
        "\n",
        "**What is a Decorator?**\\\n",
        "In Python, a **decorator** is a function that modifies the behavior of another function. The `@` syntax is used to apply a decorator.\n",
        "\n",
        "\n",
        "**`@app.get('/')`**\n",
        "- Defines a **GET endpoint** at the root URL `/`.\n",
        "- When someone sends a GET request to `/`, the `home()` function will be triggered.\n",
        "- Returns a welcome message.\n",
        "- Whenever a user clicks on the deployment web app link or API link, a GET request is automatically triggered in the background and sent to the API. As defined in the function, this request targets the homepage, and the user is greeted with a welcome message.\n",
        "\n",
        "**`@app.post('/v1/customer')`**\n",
        "- Defines a **POST endpoint** at `/v1/customer`.\n",
        "- Typically used to send customer data (like JSON payload) for **churn prediction** for a **single customer**.\n",
        "- Which return the json object, as we have given \"return\" statement in the predict_churn function\n",
        "\n",
        "**`@app.post('/v1/customerbatch')`**\n",
        "- Defines another **POST endpoint**, but for **batch predictions**.\n",
        "- Sending multiple customer data points at once (e.g., a list of JSON objects) and returns churn predictions for each, as we have given \"return\" statement in the predict_churn_batch function\n"
      ],
      "metadata": {
        "id": "5zh9dSjkD-TQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary of flow of script**\n",
        "1. **Model Loading (`joblib.load`)**  \n",
        "   Loads a pre-trained ML model (`churn_prediction_model_v1_0.joblib`) trained to predict churn based on customer features.\n",
        "\n",
        "2. **Flask App Setup**  \n",
        "   Initializes a simple Flask web app (`Flask(\"Telecom Customer Churn Predictor\")`), which exposes endpoints over HTTP.\n",
        "\n",
        "3. **Root Route (`/`)**  \n",
        "   Returns a welcome message for basic API connectivity testing.\n",
        "\n",
        "4. **Single Customer Prediction (`/v1/customer`)**\n",
        "   - Accepts a **JSON payload** with a single customer's features.\n",
        "   - Converts the data into a Pandas DataFrame.\n",
        "   - Runs the model’s `.predict()` method.\n",
        "   - Returns a **human-readable result**: `\"churn\"` or `\"not churn\"`.\n",
        "\n",
        "5. **Batch Customer Prediction (`/v1/customerbatch`)**\n",
        "   - Accepts a **CSV file upload** via form-data.\n",
        "   - Reads customer records into a DataFrame.\n",
        "   - Predicts churn for **multiple customers at once**.\n",
        "   - Returns results in a dictionary: `{customer_id: prediction}` format.\n",
        "\n",
        "6. **Model Inference**\n",
        "   - Makes use of `.predict()` to infer whether customers will churn.\n",
        "\n",
        "7. **Debug Mode (`app.run(debug=True)`)**\n",
        "   - Allows hot-reloading, i.e., restart whenever you make changes to your Python code by reloading the updated version of your app without needing to manually stop and restart the server\n",
        "   - Enables useful logs during deployment\n",
        "\n",
        "**Why This Is Valuable**\n",
        "- **Real-time ML integration:** Bridges the gap between data science and real-world business applications.\n",
        "- **Scalable predictions:** Enables both real-time (single) and batch processing of customers.\n",
        "- **Automation-ready:** Can trigger automated actions (e.g., customer retention emails) based on predictions.\n"
      ],
      "metadata": {
        "id": "22J2PO_9J3FA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependencies file"
      ],
      "metadata": {
        "id": "STDSb04iT-rL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile backend_files/requirements.txt\n",
        "pandas==2.2.2\n",
        "numpy==2.0.2\n",
        "scikit-learn==1.6.1\n",
        "xgboost==2.1.4\n",
        "joblib==1.4.2\n",
        "Werkzeug==2.2.2\n",
        "flask==2.2.2\n",
        "gunicorn==20.1.0\n",
        "requests==2.28.1\n",
        "uvicorn[standard]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKj1zTp-FJm4",
        "outputId": "993c4730-0a8d-4937-ee99-a36f1f66a57c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing backend_files/requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **`flask==2.2.2`**\n",
        "- Flask is a lightweight and flexible web framework for Python that allows developers to build web applications quickly and easily. It is designed with simplicity in mind, providing the essentials to get a web app up and running without unnecessary complexity. Flask supports extensions for added functionality and is known for its ease of use, making it a popular choice for both beginners and experienced developers. For more information, click on the link [here](https://flask.palletsprojects.com/en/stable/)\n",
        "\n",
        "---\n",
        "\n",
        "2. **`Werkzeug==2.2.2`**\n",
        "- Werkzeug is a comprehensive WSGI (Web Server Gateway Interface) utility library for Python, designed to help build web applications in a flexible and modular way. It provides tools for request and response handling, routing, and session management, among other features. Often used as a foundation for popular web frameworks like Flask, Werkzeug allows developers to create efficient and reliable web services. For more information, click on the link [here](https://werkzeug.palletsprojects.com/en/stable/)\n",
        "\n",
        "---\n",
        "\n",
        "3. **`gunicorn==20.1.0`**\n",
        "- Gunicorn (Green Unicorn) is a web server for Python that helps run web applications. It can handle multiple requests at the same time by using several worker processes, which makes it faster and more efficient. Gunicorn is easy to set up and is often used with popular Python web frameworks like Flask and Django to make sure websites run smoothly in production. For more information, click on the link [here](https://docs.gunicorn.org/en/stable/)\n",
        "\n",
        "---\n",
        "\n",
        "4. **`requests==2.28.1`**\n",
        "- Requests is a popular Python library that makes it easy to send HTTP requests and handle responses. It simplifies making GET, POST, PUT, and DELETE requests to web servers and makes it easy to work with APIs. With a simple and intuitive interface, Requests helps developers communicate with web services effortlessly, handling things like URL encoding, authentication, and session management. For more information, click on the link [here](https://requests.readthedocs.io/en/latest/)\n",
        "\n",
        "---\n",
        "\n",
        "5. **`uvicorn[standard]`**\n",
        "- Uvicorn is a lightning-fast ASGI (Asynchronous Server Gateway Interface) web server for Python, designed to run asynchronous web applications. It is compatible with modern frameworks like FastAPI and Starlette, enabling high-performance handling of web requests with support for both HTTP/1.1 and HTTP/2. Uvicorn is easy to set up and is ideal for building scalable web applications that require asynchronous capabilities. For more information, click on the link [here](https://www.uvicorn.org/)"
      ],
      "metadata": {
        "id": "4YS6m_cML9X0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary Table**\n",
        "\n",
        "| Package         | Role                            | Use Case                        |\n",
        "|----------------|----------------------------------|----------------------------------|\n",
        "| **Flask**       | Web framework                   | Build API                        |\n",
        "| **Werkzeug**    | WSGI engine, routing, utils     | Used internally by Flask         |\n",
        "| **Gunicorn**    | WSGI server                     | Run Flask in production          |\n",
        "| **Requests**    | HTTP client                     | Call/test APIs                   |\n",
        "| **Uvicorn**     | ASGI server                     | Run async applications      |"
      ],
      "metadata": {
        "id": "d9tmAY1U-I3d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dockerfile"
      ],
      "metadata": {
        "id": "JWD7rPCRUEtD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile backend_files/Dockerfile\n",
        "FROM python:3.9-slim\n",
        "\n",
        "# Set the working directory inside the container\n",
        "WORKDIR /app\n",
        "\n",
        "# Copy all files from the current directory to the container's working directory\n",
        "COPY . .\n",
        "\n",
        "# Install dependencies from the requirements file without using cache to reduce image size\n",
        "RUN pip install --no-cache-dir -r requirements.txt\n",
        "\n",
        "# Define the command to start the application using Gunicorn with 4 worker processes\n",
        "# - `-w 4`: Uses 4 worker processes for handling requests\n",
        "# - `-b 0.0.0.0:7860`: Binds the server to port 7860 on all network interfaces\n",
        "# - `app:app`: Runs the Flask app (assuming `app.py` contains the Flask instance named `app`)\n",
        "CMD [\"gunicorn\", \"-w\", \"4\", \"-b\", \"0.0.0.0:7860\", \"app:app\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fujFE1-fFJkY",
        "outputId": "5fa5668e-9c2f-45be-c610-3e008eaac62b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing backend_files/Dockerfile\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Dockerfile is used to **containerize your Flask application**, making it easy to deploy and run consistently across different environments. Let’s walk through what each line does and why it’s important:\n",
        "\n",
        "---\n",
        "\n",
        "**Line-by-Line Explanation**\n",
        "\n",
        "`FROM python:3.9-slim`\n",
        "- **What:** Starts from a lightweight version of Python 3.9; is fetched from [Docker Hub](https://hub.docker.com/layers/library/python/3.9-slim/images/sha256-b370e60efdfcc5fcb0a080c0905bbcbeb1060db3ce07c3ea0e830b0d4a17f758)\n",
        "- **Why:** It keeps the image small and efficient while still having everything needed to run Python apps.\n",
        "\n",
        "---\n",
        "\n",
        "`WORKDIR /app`\n",
        "- **What:** Sets the working directory inside the container to `/app`.\n",
        "- **Why:** All subsequent commands (copying files, running commands) happen inside this folder, keeping things organized.\n",
        "\n",
        "---\n",
        "\n",
        "`COPY . .`\n",
        "- **What:** Copies everything from your local project directory into the `/app` folder inside the container.\n",
        "- **Why:** This ensures your source code, models, and configuration files are available in the container.\n",
        "\n",
        "---\n",
        "\n",
        "`RUN pip install --no-cache-dir -r requirements.txt`\n",
        "- **What:** Installs all Python dependencies listed in `requirements.txt`.\n",
        "- **Why:** These are needed to run your Flask app, and using `--no-cache-dir` keeps the image smaller by not storing temporary installation files.\n",
        "\n",
        "---\n",
        "\n",
        "`CMD [\"gunicorn\", \"-w\", \"4\", \"-b\", \"0.0.0.0:7860\", \"app:app\"]`\n",
        "- **What:** This tells the container what command to run when it starts:\n",
        "  - `gunicorn` is a production-grade Python WSGI server.\n",
        "  - `-w 4` means it will use 4 worker processes (good for handling multiple requests).\n",
        "  - `-b 0.0.0.0:7860` binds the app to all IP addresses on port 7860.\n",
        "  - `app:app` means it will look for a file called `app.py`, and within it, the Flask instance named `app`.\n",
        "\n",
        "- **Why:** Gunicorn is preferred over Flask’s development server for production use. It handles multiple users more efficiently and securely.\n",
        "\n",
        "---\n",
        "\n",
        "**What This Dockerfile Achieves**\n",
        "- Packages your Flask app and dependencies into a single container.\n",
        "- Ensures consistency across environments (e.g., local machine, cloud server, Hugging Face Space).\n",
        "- Uses Gunicorn for scalable, production-ready deployment."
      ],
      "metadata": {
        "id": "JlClOMxLTryA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Uploading Files to Hugging Face Space for the Backend"
      ],
      "metadata": {
        "id": "_Z4mO86r6nxN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**: Before running the code below, ensure that the serialized ML model has been uploaded in to `backend_files` folder."
      ],
      "metadata": {
        "id": "j__3T0sS6tUb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for hugging face space authentication to upload files\n",
        "from huggingface_hub import login, HfApi\n",
        "\n",
        "access_key = \"---------------------\"  # Your Hugging Face token created from access keys in write mode\n",
        "repo_id = \"user_name/space_name\"  # Your Hugging Face space id\n",
        "\n",
        "# Login to Hugging Face platform with the access token\n",
        "login(token=access_key)\n",
        "\n",
        "# Initialize the API\n",
        "api = HfApi()\n",
        "\n",
        "# Upload Streamlit app files stored in the folder called deployment_files\n",
        "api.upload_folder(\n",
        "    folder_path=\"/content/backend_files\",  # Local folder path\n",
        "    repo_id=repo_id,  # Hugging face space id\n",
        "    repo_type=\"space\",  # Hugging face repo type \"space\"\n",
        ")"
      ],
      "metadata": {
        "id": "3oQeTq1P6uYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# App Frontend"
      ],
      "metadata": {
        "id": "rz1AJNb4N8uG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Points to note before executing the below cells\n",
        "- Create a Streamlit space on Hugging Face by following the instructions provided on the content page titled **`Creating Spaces and Adding Secrets in Hugging Face`** from Week 1"
      ],
      "metadata": {
        "id": "h-u_mePuNKlD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Streamlit for Interactive UI"
      ],
      "metadata": {
        "id": "UsCYxkq_UL3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a folder for storing the files needed for frontend UI deployment\n",
        "os.makedirs(\"frontend_files\", exist_ok=True)"
      ],
      "metadata": {
        "id": "eOcW-xasOHZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile frontend_files/app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "# Streamlit UI for Customer Churn Prediction\n",
        "st.title(\"Telecom Customer Churn Prediction App\")\n",
        "st.write(\"This tool predicts customer churn risk based on their details. Enter the required information below.\")\n",
        "\n",
        "# Collect user input based on dataset columns\n",
        "CustomerID = st.number_input(\"Customer ID\", min_value=10000000, max_value=99999999)\n",
        "SeniorCitizen = st.selectbox(\"Senior citizen\", [\"Yes\", \"No\"])\n",
        "Partner = st.selectbox(\"Does the customer have a partner?\", [\"Yes\", \"No\"])\n",
        "Dependents = st.selectbox(\"Does the customer have dependents?\", [\"Yes\", \"No\"])\n",
        "PhoneService = st.selectbox(\"Does the customer have phone service?\", [\"Yes\", \"No\"])\n",
        "InternetService = st.selectbox(\"Type of Internet Service\", [\"DSL\", \"Fiber optic\", \"No\"])\n",
        "Contract = st.selectbox(\"Type of Contract\", [\"Month-to-month\", \"One year\", \"Two year\"])\n",
        "PaymentMethod = st.selectbox(\"Payment Method\", [\"Electronic check\", \"Mailed check\", \"Bank transfer\", \"Credit card\"])\n",
        "tenure = st.number_input(\"Tenure (Months with the company)\", min_value=0, value=12)\n",
        "MonthlyCharges = st.number_input(\"Monthly Charges\", min_value=0.0, value=50.0)\n",
        "TotalCharges = st.number_input(\"Total Charges\", min_value=0.0, value=600.0)\n",
        "\n",
        "# Convert categorical inputs to match model training\n",
        "customer_data = {\n",
        "    'SeniorCitizen': 1 if SeniorCitizen == \"Yes\" else 0,\n",
        "    'Partner':Partner,\n",
        "    'Dependents': Dependents,\n",
        "    'tenure': tenure,\n",
        "    'PhoneService': PhoneService,\n",
        "    'InternetService': InternetService,\n",
        "    'Contract': Contract,\n",
        "    'PaymentMethod': PaymentMethod,\n",
        "    'MonthlyCharges': MonthlyCharges,\n",
        "    'TotalCharges': TotalCharges\n",
        "}\n",
        "\n",
        "\n",
        "if st.button(\"Predict\", type='primary'):\n",
        "    response = requests.post(\"https://<user_name>-<space_name>.hf.space/v1/customer\", json=customer_data)    # enter user name and space name before running the cell\n",
        "    if response.status_code == 200:\n",
        "        result = response.json()\n",
        "        churn_prediction = result[\"Prediction\"]  # Extract only the value\n",
        "        st.write(f\"Based on the information provided, the customer with ID {CustomerID} is likely to {churn_prediction}.\")\n",
        "    else:\n",
        "        st.error(\"Error in API request\")\n",
        "\n",
        "# Batch Prediction\n",
        "st.subheader(\"Batch Prediction\")\n",
        "\n",
        "file = st.file_uploader(\"Upload CSV file\", type=[\"csv\"])\n",
        "if file is not None:\n",
        "    if st.button(\"Predict for Batch\", type='primary'):\n",
        "        response = requests.post(\"https://<user_name>-<space_name>.hf.space/v1/customerbatch\", files={\"file\": file})    # enter user name and space name before running the cell\n",
        "        if response.status_code == 200:\n",
        "            result = response.json()\n",
        "            st.header(\"Batch Prediction Results\")\n",
        "            st.write(result)\n",
        "        else:\n",
        "            st.error(\"Error in API request\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McWDBwZPFJe6",
        "outputId": "7627c4f9-b545-4d84-ac33-aa2cf98e145d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting frontend_files/app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Purpose**\n",
        "This app allows users (business teams, analysts, etc.) to **interact with your churn prediction model** using a friendly interface - no coding needed.\n",
        "\n",
        "---\n",
        "\n",
        "**What It Contains**\n",
        "\n",
        "1. **User Inputs for a Single Customer**\n",
        "- Sends a **JSON request** to your `/v1/customer` API using `requests.post()`.\n",
        "\n",
        "---\n",
        "\n",
        "**Batch Prediction with CSV Upload**\n",
        "- Allows uploading a CSV file (e.g., list of customer records).\n",
        "- Sends it to the `/v1/customerbatch` API endpoint.\n",
        "- Displays the results in a clean dictionary-style table using `st.write()`.\n",
        "\n",
        "---\n",
        "\n",
        "**Why This UI Is Important**\n",
        "\n",
        "- **Non-technical users** can try the model easily.\n",
        "- Supports both **ad-hoc testing** and **batch predictions**.\n",
        "- Can be hosted on **Hugging Face Spaces**, **Streamlit Cloud**, or internal tools."
      ],
      "metadata": {
        "id": "jWotnA12QxQn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependencies file"
      ],
      "metadata": {
        "id": "beq1RbMhUQmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile frontend_files/requirements.txt\n",
        "pandas==2.2.2\n",
        "requests==2.28.1\n",
        "streamlit==1.43.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mRb5eXRR7u-",
        "outputId": "321109c3-67eb-4a5c-8911-3c001e73b596"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting frontend_files/requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dockerfile"
      ],
      "metadata": {
        "id": "h_SIZoHHMtqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile frontend_files/Dockerfile\n",
        "# Use a minimal base image with Python 3.9 installed\n",
        "FROM python:3.9-slim\n",
        "\n",
        "# Set the working directory inside the container to /app\n",
        "WORKDIR /app\n",
        "\n",
        "# Copy all files from the current directory on the host to the container's /app directory\n",
        "COPY . .\n",
        "\n",
        "# Install Python dependencies listed in requirements.txt\n",
        "RUN pip3 install -r requirements.txt\n",
        "\n",
        "# Define the command to run the Streamlit app on port 8501 and make it accessible externally\n",
        "CMD [\"streamlit\", \"run\", \"app.py\", \"--server.port=8501\", \"--server.address=0.0.0.0\", \"--server.enableXsrfProtection=false\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzXGKRHbMgyO",
        "outputId": "8764009b-4fad-45c4-92db-069ac12f1e67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting frontend_files/Dockerfile\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Uploading Files to Hugging Face Repository"
      ],
      "metadata": {
        "id": "GChQ7KMTXmfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "access_key = \"---------------------\"  # Your Hugging Face token created from access keys in write mode\n",
        "repo_id = \"user_name/space_name\"  # Your Hugging Face space id\n",
        "\n",
        "# Login to Hugging Face platform with the access token\n",
        "login(token=access_key)\n",
        "\n",
        "# Initialize the API\n",
        "api = HfApi()\n",
        "\n",
        "# Upload Streamlit app files stored in the folder called deployment_files\n",
        "api.upload_folder(\n",
        "    folder_path=\"/content/frontend_files\",  # Local folder path\n",
        "    repo_id=repo_id,  # Hugging face space id\n",
        "    repo_type=\"space\",  # Hugging face repo type \"space\"\n",
        ")"
      ],
      "metadata": {
        "id": "TsW6UmL9GIFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question - Why do we need streamlit app.py when we have a flask app.py to make predictions?"
      ],
      "metadata": {
        "id": "vRqCTdwrRP6S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You need both the **Flask app** and the **Streamlit app** because they serve **two different roles** in your project.\n",
        "\n",
        "The **Flask app** is your **backend**. It holds the trained machine learning model and handles the actual prediction logic. When someone sends customer data to this Flask API, it processes that data and returns a prediction—whether the customer will churn or not. But the Flask app by itself doesn’t offer any user interface; it just waits for data, makes predictions, and sends back a response in code form (like JSON).\n",
        "\n",
        "The **Streamlit app**, on the other hand, is your **frontend**. It’s built for people to interact with—especially non-technical users. It provides a clean interface with sliders, dropdowns, and file upload buttons. Users can input customer details or upload a CSV file, and the Streamlit app will send that data to the Flask API in the background. When it receives the prediction, it presents the result in a user-friendly way—like showing “This customer is likely to churn.”\n",
        "\n",
        "So, in short:\n",
        "- **Flask** does the smart stuff in the background.\n",
        "- **Streamlit** helps users talk to that smart system without needing to write any code.\n",
        "\n",
        "---\n",
        "\n",
        "**Frontend–Backend Workflow**\n",
        "\n",
        "| Frontend (Streamlit)                 | Backend (Flask API)                |\n",
        "|-------------------------------------|------------------------------------|\n",
        "| Collects user input                 | Receives JSON or CSV               |\n",
        "| Converts input to correct format    | Parses input using `request.get_json()` or `request.files` |\n",
        "| Sends `POST` request via `requests` | Runs the model + returns result    |\n",
        "| Displays prediction in human-readable form                 | Returns `\"churn\"` or `\"not churn\"` |"
      ],
      "metadata": {
        "id": "Zgo_Qz09ShX_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inferencing using Flask API\n"
      ],
      "metadata": {
        "id": "uL3mB7rziwnz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets see how to interact with a deployed Flask API using programatically on Hugging Face Spaces to perform **online** and **batch inference**  \n",
        "\n",
        "We will:  \n",
        "1. Send API requests for both online and batch inference.  \n",
        "2. Process and visualize the model predictions.  \n",
        "\n",
        "This ensures seamless interaction with the deployed model while leveraging the API for scalable inference."
      ],
      "metadata": {
        "id": "vXCEJGmzI_zS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9g9MT90dyLS"
      },
      "outputs": [],
      "source": [
        "import json  # To handle JSON formatting for API requests and responses\n",
        "import requests  # To send HTTP requests to the deployed Flask API\n",
        "\n",
        "import pandas as pd  # For data manipulation and analysis\n",
        "import numpy as np  # For numerical computations\n",
        "\n",
        "from sklearn.model_selection import train_test_split  # To split data for batch inference scenarios"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_root_url = \"https://<user_name>-<space_name>.hf.space/\"  # Base URL of the deployed Flask API on Hugging Face Spaces"
      ],
      "metadata": {
        "id": "jJx0262Ld6bD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_url = model_root_url + \"/v1/customer\"  # Endpoint for online (single) inference"
      ],
      "metadata": {
        "id": "Kq6LpmrjeITI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since our model predictions are provided by the following endpoint we created using Flask, we need to invoke the same to make prediction.\n",
        "\n",
        "> ```@app.post('/v1/customer')```"
      ],
      "metadata": {
        "id": "l9oJ16-b1nE5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_batch_url = model_root_url + \"/v1/customerbatch\"  # Endpoint for batch inference"
      ],
      "metadata": {
        "id": "0SzGv3INkDHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ```@app.post('/v1/customerbatch')```"
      ],
      "metadata": {
        "id": "sWstFGdDIJ4s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Online"
      ],
      "metadata": {
        "id": "dPUTD0Snew7n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Online Inference:** Sending a single request to the API and receiving an immediate response. This is useful for real-time applications like recommendation systems and fraud detection.  \n",
        "* This data is sent as a JSON payload in a POST request to the model endpoint.\n",
        "\n",
        "* The model processes the input features and returns a prediction."
      ],
      "metadata": {
        "id": "fhhivXJKFEfg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "payload = {\n",
        "  'SeniorCitizen': 0,\n",
        " 'Partner': 'Yes',\n",
        " 'Dependents': 'No',\n",
        " 'tenure': 63,\n",
        " 'PhoneService': 'No',\n",
        " 'InternetService': 'Fiber optic',\n",
        " 'Contract': 'One year',\n",
        " 'PaymentMethod': 'Credit card',\n",
        " 'MonthlyCharges': 48.66,\n",
        " 'TotalCharges': 3065.58\n",
        "}"
      ],
      "metadata": {
        "id": "hwBt4lwjeRIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sending a POST request to the model endpoint with the test payload\n",
        "response = requests.post(model_url, json=payload)"
      ],
      "metadata": {
        "id": "TEcUJhPueTg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_LRhw-wDPaQ",
        "outputId": "82051055-3620-4c99-b314-2b00ec9e52f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Response [200]>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`<Response [200]>` indicates that the HTTP request was successful.  \n",
        "\n",
        "- `Response` is the object returned by `requests.post()`.  \n",
        "- `[200]` is the HTTP status code, meaning **OK** (the request was processed successfully).  \n"
      ],
      "metadata": {
        "id": "C396WwoKDnzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.json())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5siPq2UeVL7",
        "outputId": "1af4db0b-64a9-4e26-c914-1fa8dfff4ac3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Prediction': 'churn'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`response.json()` is used to parse the response body as JSON.  \n",
        "- If the API returns data in JSON format, `.json()` converts it into a Python dictionary.  \n",
        "- This allows easy access to specific values using keys.  "
      ],
      "metadata": {
        "id": "NFLfl-c-EfgP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch"
      ],
      "metadata": {
        "id": "3mU64zPDeyKR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Batch Inference:** Sending multiple inputs in a single request, allowing efficient processing of large datasets. This is ideal for analyzing historical data at scale."
      ],
      "metadata": {
        "id": "aOEQQlw_FG6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  batch_churn_dataset = pd.read_csv(\"Batch-data-telecom.csv\")"
      ],
      "metadata": {
        "id": "IdPUvXEGeki5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare batch input for API request\n",
        "batch_input = {\n",
        "    'file': batch_churn_dataset.to_csv(header=True, index=False).encode('utf-8')\n",
        "}"
      ],
      "metadata": {
        "id": "i6XXra-zfUGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code prepares the customer churn data as a **file-like object** to send in an API request (in the `files` parameter of `requests.post`).\n",
        "\n",
        "- `batch_churn_dataset`:  \n",
        "  This is a Pandas DataFrame containing customer data loaded from a CSV file:\n",
        "\n",
        "- `.to_csv(header=True, index=False)`:  \n",
        "  Converts the DataFrame into a CSV **string**, keeping the column headers (`header=True`) but **excluding the index** (`index=False`).\n",
        "\n",
        "- `.encode('utf-8')`:  \n",
        "  Encodes the CSV string into **bytes** (UTF-8 format), which is necessary for sending it over an HTTP request.\n",
        "\n",
        "- `'file': ...`:  \n",
        "  Wraps the encoded CSV data in a dictionary with the key `'file'`, which is expected by the FlaskAPI backend using `UploadFile`."
      ],
      "metadata": {
        "id": "k3wqs3JDIu4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Send request to the model API for batch predictions\n",
        "response = requests.post(\n",
        "    model_batch_url,  # Model endpoint URL\n",
        "    files=batch_input\n",
        ")"
      ],
      "metadata": {
        "id": "pWeW9HKafVxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This line sends a **POST request** to the deployed model API to perform **batch churn prediction**.\n",
        "\n",
        "- `requests.post(...)`:  \n",
        "  This uses the `requests` library to make a **POST** request to a web server (in this case, our API endpoint for batch predictions).\n",
        "\n",
        "- `model_batch_url`:  \n",
        "  This is the URL endpoint where the batch prediction API is hosted. It should look like this:  \n",
        "  `\"https://<user_name>-<space_name>.hf.space/v1/customerbatch\"`\n",
        "\n",
        "- `files=batch_input`:  \n",
        "  This sends the batch data file (e.g., a CSV) as part of the request using the `files` parameter.  \n",
        "\n",
        "- `response`:  \n",
        "  This is the result of the API call and contains the response from the server, including predicted churn values."
      ],
      "metadata": {
        "id": "yvuvdhacJNjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToTKY0TWILky",
        "outputId": "4bfae874-4ee6-4d4e-e90b-813422173e83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Response [200]>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract predictions from API response\n",
        "response.text"
      ],
      "metadata": {
        "id": "WHxcRKFhfXlc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "d42e85b0-60ec-4056-9f3d-95f271da038b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{\"CUST0048\":\"Not Churn\",\"CUST0049\":\"Churn\",\"CUST0050\":\"Not Churn\",\"CUST0051\":\"Churn\",\"CUST0052\":\"Churn\",\"CUST0053\":\"Not Churn\",\"CUST0054\":\"Churn\",\"CUST0055\":\"Not Churn\",\"CUST0056\":\"Churn\",\"CUST0057\":\"Churn\",\"CUST0058\":\"Churn\"}\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, we receive a JSON where each key represents a customer ID, and the value represents the model prediction of whether the customer will churn or not."
      ],
      "metadata": {
        "id": "Z721wU8kuXG1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size=6 color=\"blue\">Power Ahead!</font>\n",
        "___"
      ],
      "metadata": {
        "id": "-O17xan3ubRL"
      }
    }
  ]
}